{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    2022-08-10 11:05:10.094713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
    "    2022-08-10 11:05:10.094738: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    df=pd.read_csv(\"/home/divum/ML/DATASETS/Churn_Modelling.csv\")\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[3\\]:\n",
    "\n",
    "|      | RowNumber | CustomerId | Surname   | CreditScore | Geography | Gender | Age | Tenure | Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "|------|-----------|------------|-----------|-------------|-----------|--------|-----|--------|-----------|---------------|-----------|----------------|-----------------|--------|\n",
    "| 0    | 1         | 15634602   | Hargrave  | 619         | France    | Female | 42  | 2      | 0.00      | 1             | 1         | 1              | 101348.88       | 1      |\n",
    "| 1    | 2         | 15647311   | Hill      | 608         | Spain     | Female | 41  | 1      | 83807.86  | 1             | 0         | 1              | 112542.58       | 0      |\n",
    "| 2    | 3         | 15619304   | Onio      | 502         | France    | Female | 42  | 8      | 159660.80 | 3             | 1         | 0              | 113931.57       | 1      |\n",
    "| 3    | 4         | 15701354   | Boni      | 699         | France    | Female | 39  | 1      | 0.00      | 2             | 0         | 0              | 93826.63        | 0      |\n",
    "| 4    | 5         | 15737888   | Mitchell  | 850         | Spain     | Female | 43  | 2      | 125510.82 | 1             | 1         | 1              | 79084.10        | 0      |\n",
    "| ...  | ...       | ...        | ...       | ...         | ...       | ...    | ... | ...    | ...       | ...           | ...       | ...            | ...             | ...    |\n",
    "| 9995 | 9996      | 15606229   | Obijiaku  | 771         | France    | Male   | 39  | 5      | 0.00      | 2             | 1         | 0              | 96270.64        | 0      |\n",
    "| 9996 | 9997      | 15569892   | Johnstone | 516         | France    | Male   | 35  | 10     | 57369.61  | 1             | 1         | 1              | 101699.77       | 0      |\n",
    "| 9997 | 9998      | 15584532   | Liu       | 709         | France    | Female | 36  | 7      | 0.00      | 1             | 0         | 1              | 42085.58        | 1      |\n",
    "| 9998 | 9999      | 15682355   | Sabbatini | 772         | Germany   | Male   | 42  | 3      | 75075.31  | 2             | 1         | 0              | 92888.52        | 1      |\n",
    "| 9999 | 10000     | 15628319   | Walker    | 792         | France    | Female | 28  | 4      | 130142.79 | 1             | 1         | 0              | 38190.78        | 0      |\n",
    "\n",
    "10000 rows × 14 columns\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # df.drop('CustomerId',axis='columns',inplace=True)\n",
    "    df.drop('RowNumber',axis='columns',inplace=True)\n",
    "    df.drop('Surname',axis='columns',inplace=True)\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "|      | CreditScore | Geography | Gender | Age | Tenure | Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "|------|-------------|-----------|--------|-----|--------|-----------|---------------|-----------|----------------|-----------------|--------|\n",
    "| 0    | 619         | France    | Female | 42  | 2      | 0.00      | 1             | 1         | 1              | 101348.88       | 1      |\n",
    "| 1    | 608         | Spain     | Female | 41  | 1      | 83807.86  | 1             | 0         | 1              | 112542.58       | 0      |\n",
    "| 2    | 502         | France    | Female | 42  | 8      | 159660.80 | 3             | 1         | 0              | 113931.57       | 1      |\n",
    "| 3    | 699         | France    | Female | 39  | 1      | 0.00      | 2             | 0         | 0              | 93826.63        | 0      |\n",
    "| 4    | 850         | Spain     | Female | 43  | 2      | 125510.82 | 1             | 1         | 1              | 79084.10        | 0      |\n",
    "| ...  | ...         | ...       | ...    | ... | ...    | ...       | ...           | ...       | ...            | ...             | ...    |\n",
    "| 9995 | 771         | France    | Male   | 39  | 5      | 0.00      | 2             | 1         | 0              | 96270.64        | 0      |\n",
    "| 9996 | 516         | France    | Male   | 35  | 10     | 57369.61  | 1             | 1         | 1              | 101699.77       | 0      |\n",
    "| 9997 | 709         | France    | Female | 36  | 7      | 0.00      | 1             | 0         | 1              | 42085.58        | 1      |\n",
    "| 9998 | 772         | Germany   | Male   | 42  | 3      | 75075.31  | 2             | 1         | 0              | 92888.52        | 1      |\n",
    "| 9999 | 792         | France    | Female | 28  | 4      | 130142.79 | 1             | 1         | 0              | 38190.78        | 0      |\n",
    "\n",
    "10000 rows × 11 columns\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    df.dtypes\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "    CreditScore          int64\n",
    "    Geography           object\n",
    "    Gender              object\n",
    "    Age                  int64\n",
    "    Tenure               int64\n",
    "    Balance            float64\n",
    "    NumOfProducts        int64\n",
    "    HasCrCard            int64\n",
    "    IsActiveMember       int64\n",
    "    EstimatedSalary    float64\n",
    "    Exited               int64\n",
    "    dtype: object\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    df.CreditScore.unique()\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "    array([619, 608, 502, 699, 850, 645, 822, 376, 501, 684, 528, 497, 476,\n",
    "           549, 635, 616, 653, 587, 726, 732, 636, 510, 669, 846, 577, 756,\n",
    "           571, 574, 411, 591, 533, 553, 520, 722, 475, 490, 804, 582, 472,\n",
    "           465, 556, 834, 660, 776, 829, 637, 550, 698, 585, 788, 655, 601,\n",
    "           656, 725, 511, 614, 742, 687, 555, 603, 751, 581, 735, 661, 675,\n",
    "           738, 813, 657, 604, 519, 664, 678, 757, 416, 665, 777, 543, 506,\n",
    "           493, 652, 750, 729, 646, 647, 808, 524, 769, 730, 515, 773, 814,\n",
    "           710, 413, 623, 670, 622, 785, 605, 479, 685, 538, 562, 721, 628,\n",
    "           668, 828, 674, 625, 432, 770, 758, 795, 686, 789, 589, 461, 584,\n",
    "           579, 663, 682, 793, 691, 485, 650, 754, 535, 716, 539, 706, 586,\n",
    "           631, 717, 800, 683, 704, 615, 667, 484, 480, 578, 512, 606, 597,\n",
    "           778, 514, 525, 715, 580, 807, 521, 759, 516, 711, 618, 643, 671,\n",
    "           689, 620, 676, 572, 695, 592, 567, 694, 547, 594, 673, 610, 767,\n",
    "           763, 712, 703, 662, 659, 523, 772, 545, 634, 739, 771, 681, 544,\n",
    "           696, 766, 727, 693, 557, 531, 498, 651, 791, 733, 811, 707, 714,\n",
    "           782, 775, 799, 602, 744, 588, 747, 583, 627, 731, 629, 438, 642,\n",
    "           806, 474, 559, 429, 680, 749, 734, 644, 626, 649, 805, 718, 840,\n",
    "           630, 654, 762, 568, 613, 522, 737, 648, 443, 640, 540, 460, 593,\n",
    "           801, 611, 802, 745, 483, 690, 492, 709, 705, 560, 752, 701, 537,\n",
    "           487, 596, 702, 486, 724, 548, 464, 790, 534, 748, 494, 590, 468,\n",
    "           509, 818, 816, 536, 753, 774, 621, 569, 658, 798, 641, 542, 692,\n",
    "           639, 765, 570, 638, 599, 632, 779, 527, 564, 833, 504, 842, 508,\n",
    "           417, 598, 741, 607, 761, 848, 546, 439, 755, 760, 526, 713, 700,\n",
    "           666, 566, 495, 688, 612, 477, 427, 839, 819, 720, 459, 503, 624,\n",
    "           529, 563, 482, 796, 445, 746, 786, 554, 672, 787, 499, 844, 450,\n",
    "           815, 838, 803, 736, 633, 600, 679, 517, 792, 743, 488, 421, 841,\n",
    "           708, 507, 505, 456, 435, 561, 518, 565, 728, 784, 552, 609, 764,\n",
    "           697, 723, 551, 444, 719, 496, 541, 830, 812, 677, 420, 595, 617,\n",
    "           809, 500, 826, 434, 513, 478, 797, 363, 399, 463, 780, 452, 575,\n",
    "           837, 794, 824, 428, 823, 781, 849, 489, 431, 457, 768, 831, 359,\n",
    "           820, 573, 576, 558, 817, 449, 440, 415, 821, 530, 350, 446, 425,\n",
    "           740, 481, 783, 358, 845, 451, 458, 469, 423, 404, 836, 473, 835,\n",
    "           466, 491, 351, 827, 843, 365, 532, 414, 453, 471, 401, 810, 832,\n",
    "           470, 447, 422, 825, 430, 436, 426, 408, 847, 418, 437, 410, 454,\n",
    "           407, 455, 462, 386, 405, 383, 395, 467, 433, 442, 424, 448, 441,\n",
    "           367, 412, 382, 373, 419])\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    df.Geography.unique()\n",
    "\n",
    "Out\\[10\\]:\n",
    "\n",
    "    array(['France', 'Spain', 'Germany'], dtype=object)\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    def uni(d):\n",
    "        for col in d:\n",
    "            if df[col].dtypes=='object':\n",
    "                print(col,\":\",d[col].unique())\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    uni(df)\n",
    "\n",
    "    Geography : ['France' 'Spain' 'Germany']\n",
    "    Gender : ['Female' 'Male']\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    df.replace({'Female':0,'Male':1},inplace=True)\n",
    "    df.replace({'France':0,'Spain':1,'Germany':2},inplace=True)\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "Out\\[20\\]:\n",
    "\n",
    "    CreditScore        0\n",
    "    Geography          0\n",
    "    Gender             0\n",
    "    Age                0\n",
    "    Tenure             0\n",
    "    Balance            0\n",
    "    NumOfProducts      0\n",
    "    HasCrCard          0\n",
    "    IsActiveMember     0\n",
    "    EstimatedSalary    0\n",
    "    Exited             0\n",
    "    dtype: int64\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    for col in df:\n",
    "        print(col,\":\",df[col].unique())\n",
    "\n",
    "    CreditScore : [619 608 502 699 850 645 822 376 501 684 528 497 476 549 635 616 653 587\n",
    "     726 732 636 510 669 846 577 756 571 574 411 591 533 553 520 722 475 490\n",
    "     804 582 472 465 556 834 660 776 829 637 550 698 585 788 655 601 656 725\n",
    "     511 614 742 687 555 603 751 581 735 661 675 738 813 657 604 519 664 678\n",
    "     757 416 665 777 543 506 493 652 750 729 646 647 808 524 769 730 515 773\n",
    "     814 710 413 623 670 622 785 605 479 685 538 562 721 628 668 828 674 625\n",
    "     432 770 758 795 686 789 589 461 584 579 663 682 793 691 485 650 754 535\n",
    "     716 539 706 586 631 717 800 683 704 615 667 484 480 578 512 606 597 778\n",
    "     514 525 715 580 807 521 759 516 711 618 643 671 689 620 676 572 695 592\n",
    "     567 694 547 594 673 610 767 763 712 703 662 659 523 772 545 634 739 771\n",
    "     681 544 696 766 727 693 557 531 498 651 791 733 811 707 714 782 775 799\n",
    "     602 744 588 747 583 627 731 629 438 642 806 474 559 429 680 749 734 644\n",
    "     626 649 805 718 840 630 654 762 568 613 522 737 648 443 640 540 460 593\n",
    "     801 611 802 745 483 690 492 709 705 560 752 701 537 487 596 702 486 724\n",
    "     548 464 790 534 748 494 590 468 509 818 816 536 753 774 621 569 658 798\n",
    "     641 542 692 639 765 570 638 599 632 779 527 564 833 504 842 508 417 598\n",
    "     741 607 761 848 546 439 755 760 526 713 700 666 566 495 688 612 477 427\n",
    "     839 819 720 459 503 624 529 563 482 796 445 746 786 554 672 787 499 844\n",
    "     450 815 838 803 736 633 600 679 517 792 743 488 421 841 708 507 505 456\n",
    "     435 561 518 565 728 784 552 609 764 697 723 551 444 719 496 541 830 812\n",
    "     677 420 595 617 809 500 826 434 513 478 797 363 399 463 780 452 575 837\n",
    "     794 824 428 823 781 849 489 431 457 768 831 359 820 573 576 558 817 449\n",
    "     440 415 821 530 350 446 425 740 481 783 358 845 451 458 469 423 404 836\n",
    "     473 835 466 491 351 827 843 365 532 414 453 471 401 810 832 470 447 422\n",
    "     825 430 436 426 408 847 418 437 410 454 407 455 462 386 405 383 395 467\n",
    "     433 442 424 448 441 367 412 382 373 419]\n",
    "    Geography : [0 1 2]\n",
    "    Gender : [0 1]\n",
    "    Age : [42 41 39 43 44 50 29 27 31 24 34 25 35 45 58 32 38 46 36 33 40 51 61 49\n",
    "     37 19 66 56 26 21 55 75 22 30 28 65 48 52 57 73 47 54 72 20 67 79 62 53\n",
    "     80 59 68 23 60 70 63 64 18 82 69 74 71 76 77 88 85 84 78 81 92 83]\n",
    "    Tenure : [ 2  1  8  7  4  6  3 10  5  9  0]\n",
    "    Balance : [     0.    83807.86 159660.8  ...  57369.61  75075.31 130142.79]\n",
    "    NumOfProducts : [1 3 2 4]\n",
    "    HasCrCard : [1 0]\n",
    "    IsActiveMember : [1 0]\n",
    "    EstimatedSalary : [101348.88 112542.58 113931.57 ...  42085.58  92888.52  38190.78]\n",
    "    Exited : [1 0]\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    df.dtypes\n",
    "\n",
    "Out\\[22\\]:\n",
    "\n",
    "    CreditScore          int64\n",
    "    Geography            int64\n",
    "    Gender               int64\n",
    "    Age                  int64\n",
    "    Tenure               int64\n",
    "    Balance            float64\n",
    "    NumOfProducts        int64\n",
    "    HasCrCard            int64\n",
    "    IsActiveMember       int64\n",
    "    EstimatedSalary    float64\n",
    "    Exited               int64\n",
    "    dtype: object\n",
    "\n",
    "In \\[27\\]:\n",
    "\n",
    "    col_to_scale=[\"CreditScore\",\"Balance\",\"EstimatedSalary\",\"Balance\"]\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler=MinMaxScaler()\n",
    "    df[col_to_scale]=scaler.fit_transform(df[col_to_scale])\n",
    "\n",
    "In \\[28\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[28\\]:\n",
    "\n",
    "|      | CreditScore | Geography | Gender | Age | Tenure | Balance  | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "|------|-------------|-----------|--------|-----|--------|----------|---------------|-----------|----------------|-----------------|--------|\n",
    "| 0    | 0.538       | 0         | 0      | 42  | 2      | 0.000000 | 1             | 1         | 1              | 0.506735        | 1      |\n",
    "| 1    | 0.516       | 1         | 0      | 41  | 1      | 0.334031 | 1             | 0         | 1              | 0.562709        | 0      |\n",
    "| 2    | 0.304       | 0         | 0      | 42  | 8      | 0.636357 | 3             | 1         | 0              | 0.569654        | 1      |\n",
    "| 3    | 0.698       | 0         | 0      | 39  | 1      | 0.000000 | 2             | 0         | 0              | 0.469120        | 0      |\n",
    "| 4    | 1.000       | 1         | 0      | 43  | 2      | 0.500246 | 1             | 1         | 1              | 0.395400        | 0      |\n",
    "| ...  | ...         | ...       | ...    | ... | ...    | ...      | ...           | ...       | ...            | ...             | ...    |\n",
    "| 9995 | 0.842       | 0         | 1      | 39  | 5      | 0.000000 | 2             | 1         | 0              | 0.481341        | 0      |\n",
    "| 9996 | 0.332       | 0         | 1      | 35  | 10     | 0.228657 | 1             | 1         | 1              | 0.508490        | 0      |\n",
    "| 9997 | 0.718       | 0         | 0      | 36  | 7      | 0.000000 | 1             | 0         | 1              | 0.210390        | 1      |\n",
    "| 9998 | 0.844       | 2         | 1      | 42  | 3      | 0.299226 | 2             | 1         | 0              | 0.464429        | 1      |\n",
    "| 9999 | 0.884       | 0         | 0      | 28  | 4      | 0.518708 | 1             | 1         | 0              | 0.190914        | 0      |\n",
    "\n",
    "10000 rows × 11 columns\n",
    "\n",
    "In \\[29\\]:\n",
    "\n",
    "    for col in df:\n",
    "        print(col,\":\",df[col].unique())\n",
    "\n",
    "    CreditScore : [0.538 0.516 0.304 0.698 1.    0.59  0.944 0.052 0.302 0.668 0.356 0.294\n",
    "     0.252 0.398 0.57  0.532 0.606 0.474 0.752 0.764 0.572 0.32  0.638 0.992\n",
    "     0.454 0.812 0.442 0.448 0.122 0.482 0.366 0.406 0.34  0.744 0.25  0.28\n",
    "     0.908 0.464 0.244 0.23  0.412 0.968 0.62  0.852 0.958 0.574 0.4   0.696\n",
    "     0.47  0.876 0.61  0.502 0.612 0.75  0.322 0.528 0.784 0.674 0.41  0.506\n",
    "     0.802 0.462 0.77  0.622 0.65  0.776 0.926 0.614 0.508 0.338 0.628 0.656\n",
    "     0.814 0.132 0.63  0.854 0.386 0.312 0.286 0.604 0.8   0.758 0.592 0.594\n",
    "     0.916 0.348 0.838 0.76  0.33  0.846 0.928 0.72  0.126 0.546 0.64  0.544\n",
    "     0.87  0.51  0.258 0.67  0.376 0.424 0.742 0.556 0.636 0.956 0.648 0.55\n",
    "     0.164 0.84  0.816 0.89  0.672 0.878 0.478 0.222 0.468 0.458 0.626 0.664\n",
    "     0.886 0.682 0.27  0.6   0.808 0.37  0.732 0.378 0.712 0.472 0.562 0.734\n",
    "     0.9   0.666 0.708 0.53  0.634 0.268 0.26  0.456 0.324 0.512 0.494 0.856\n",
    "     0.328 0.35  0.73  0.46  0.914 0.342 0.818 0.332 0.722 0.536 0.586 0.642\n",
    "     0.678 0.54  0.652 0.444 0.69  0.484 0.434 0.688 0.394 0.488 0.646 0.52\n",
    "     0.834 0.826 0.724 0.706 0.624 0.618 0.346 0.844 0.39  0.568 0.778 0.842\n",
    "     0.662 0.388 0.692 0.832 0.754 0.686 0.414 0.362 0.296 0.602 0.882 0.766\n",
    "     0.922 0.714 0.728 0.864 0.85  0.898 0.504 0.788 0.476 0.794 0.466 0.554\n",
    "     0.762 0.558 0.176 0.584 0.912 0.248 0.418 0.158 0.66  0.798 0.768 0.588\n",
    "     0.552 0.598 0.91  0.736 0.98  0.56  0.608 0.824 0.436 0.526 0.344 0.774\n",
    "     0.596 0.186 0.58  0.38  0.22  0.486 0.902 0.522 0.904 0.79  0.266 0.68\n",
    "     0.284 0.718 0.71  0.42  0.804 0.702 0.374 0.274 0.492 0.704 0.272 0.748\n",
    "     0.396 0.228 0.88  0.368 0.796 0.288 0.48  0.236 0.318 0.936 0.932 0.372\n",
    "     0.806 0.848 0.542 0.438 0.616 0.896 0.582 0.384 0.684 0.578 0.83  0.44\n",
    "     0.576 0.498 0.564 0.858 0.354 0.428 0.966 0.308 0.984 0.316 0.134 0.496\n",
    "     0.782 0.514 0.822 0.996 0.392 0.178 0.81  0.82  0.352 0.726 0.7   0.632\n",
    "     0.432 0.29  0.676 0.524 0.254 0.154 0.978 0.938 0.74  0.218 0.306 0.548\n",
    "     0.358 0.426 0.264 0.892 0.19  0.792 0.872 0.408 0.644 0.874 0.298 0.988\n",
    "     0.2   0.93  0.976 0.906 0.772 0.566 0.5   0.658 0.334 0.884 0.786 0.276\n",
    "     0.142 0.982 0.716 0.314 0.31  0.212 0.17  0.422 0.336 0.43  0.756 0.868\n",
    "     0.404 0.518 0.828 0.694 0.746 0.402 0.188 0.738 0.292 0.382 0.96  0.924\n",
    "     0.654 0.14  0.49  0.534 0.918 0.3   0.952 0.168 0.326 0.256 0.894 0.026\n",
    "     0.098 0.226 0.86  0.204 0.45  0.974 0.888 0.948 0.156 0.946 0.862 0.998\n",
    "     0.278 0.162 0.214 0.836 0.962 0.018 0.94  0.446 0.452 0.416 0.934 0.198\n",
    "     0.18  0.13  0.942 0.36  0.    0.192 0.15  0.78  0.262 0.866 0.016 0.99\n",
    "     0.202 0.216 0.238 0.146 0.108 0.972 0.246 0.97  0.232 0.282 0.002 0.954\n",
    "     0.986 0.03  0.364 0.128 0.206 0.242 0.102 0.92  0.964 0.24  0.194 0.144\n",
    "     0.95  0.16  0.172 0.152 0.116 0.994 0.136 0.174 0.12  0.208 0.114 0.21\n",
    "     0.224 0.072 0.11  0.066 0.09  0.234 0.166 0.184 0.148 0.196 0.182 0.034\n",
    "     0.124 0.064 0.046 0.138]\n",
    "    Geography : [0 1 2]\n",
    "    Gender : [0 1]\n",
    "    Age : [42 41 39 43 44 50 29 27 31 24 34 25 35 45 58 32 38 46 36 33 40 51 61 49\n",
    "     37 19 66 56 26 21 55 75 22 30 28 65 48 52 57 73 47 54 72 20 67 79 62 53\n",
    "     80 59 68 23 60 70 63 64 18 82 69 74 71 76 77 88 85 84 78 81 92 83]\n",
    "    Tenure : [ 2  1  8  7  4  6  3 10  5  9  0]\n",
    "    Balance : [0.         0.33403148 0.63635718 ... 0.22865702 0.29922631 0.51870777]\n",
    "    NumOfProducts : [1 3 2 4]\n",
    "    HasCrCard : [1 0]\n",
    "    IsActiveMember : [1 0]\n",
    "    EstimatedSalary : [0.50673489 0.56270874 0.56965435 ... 0.21039009 0.46442905 0.19091423]\n",
    "    Exited : [1 0]\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    x=df.drop(\"Exited\",axis='columns')\n",
    "    y=df[\"Exited\"]\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    x\n",
    "\n",
    "Out\\[31\\]:\n",
    "\n",
    "|      | CreditScore | Geography | Gender | Age | Tenure | Balance  | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary |\n",
    "|------|-------------|-----------|--------|-----|--------|----------|---------------|-----------|----------------|-----------------|\n",
    "| 0    | 0.538       | 0         | 0      | 42  | 2      | 0.000000 | 1             | 1         | 1              | 0.506735        |\n",
    "| 1    | 0.516       | 1         | 0      | 41  | 1      | 0.334031 | 1             | 0         | 1              | 0.562709        |\n",
    "| 2    | 0.304       | 0         | 0      | 42  | 8      | 0.636357 | 3             | 1         | 0              | 0.569654        |\n",
    "| 3    | 0.698       | 0         | 0      | 39  | 1      | 0.000000 | 2             | 0         | 0              | 0.469120        |\n",
    "| 4    | 1.000       | 1         | 0      | 43  | 2      | 0.500246 | 1             | 1         | 1              | 0.395400        |\n",
    "| ...  | ...         | ...       | ...    | ... | ...    | ...      | ...           | ...       | ...            | ...             |\n",
    "| 9995 | 0.842       | 0         | 1      | 39  | 5      | 0.000000 | 2             | 1         | 0              | 0.481341        |\n",
    "| 9996 | 0.332       | 0         | 1      | 35  | 10     | 0.228657 | 1             | 1         | 1              | 0.508490        |\n",
    "| 9997 | 0.718       | 0         | 0      | 36  | 7      | 0.000000 | 1             | 0         | 1              | 0.210390        |\n",
    "| 9998 | 0.844       | 2         | 1      | 42  | 3      | 0.299226 | 2             | 1         | 0              | 0.464429        |\n",
    "| 9999 | 0.884       | 0         | 0      | 28  | 4      | 0.518708 | 1             | 1         | 0              | 0.190914        |\n",
    "\n",
    "10000 rows × 10 columns\n",
    "\n",
    "In \\[32\\]:\n",
    "\n",
    "    y\n",
    "\n",
    "Out\\[32\\]:\n",
    "\n",
    "    0       1\n",
    "    1       0\n",
    "    2       1\n",
    "    3       0\n",
    "    4       0\n",
    "           ..\n",
    "    9995    0\n",
    "    9996    0\n",
    "    9997    1\n",
    "    9998    1\n",
    "    9999    0\n",
    "    Name: Exited, Length: 10000, dtype: int64\n",
    "\n",
    "In \\[33\\]:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=28)\n",
    "\n",
    "In \\[35\\]:\n",
    "\n",
    "    xtrain.shape\n",
    "\n",
    "Out\\[35\\]:\n",
    "\n",
    "    (8000, 10)\n",
    "\n",
    "In \\[36\\]:\n",
    "\n",
    "    xtest.shape\n",
    "\n",
    "Out\\[36\\]:\n",
    "\n",
    "    (2000, 10)\n",
    "\n",
    "In \\[37\\]:\n",
    "\n",
    "    ytrain.shape\n",
    "\n",
    "Out\\[37\\]:\n",
    "\n",
    "    (8000,)\n",
    "\n",
    "In \\[38\\]:\n",
    "\n",
    "    ytest.shape\n",
    "\n",
    "Out\\[38\\]:\n",
    "\n",
    "    (2000,)\n",
    "\n",
    "In \\[43\\]:\n",
    "\n",
    "    model=keras.Sequential([\n",
    "        keras.layers.Dense(10,input_shape=(10,),activation='relu'),\n",
    "        keras.layers.Dense(100,activation='relu'),\n",
    "        keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,epochs=100)\n",
    "\n",
    "    Epoch 1/100\n",
    "    250/250 [==============================] - 0s 932us/step - loss: 0.5244 - accuracy: 0.7946\n",
    "    Epoch 2/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.7977\n",
    "    Epoch 3/100\n",
    "    250/250 [==============================] - 0s 989us/step - loss: 0.4575 - accuracy: 0.8008\n",
    "    Epoch 4/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.4518 - accuracy: 0.8039\n",
    "    Epoch 5/100\n",
    "    250/250 [==============================] - 0s 971us/step - loss: 0.4435 - accuracy: 0.8076\n",
    "    Epoch 6/100\n",
    "    250/250 [==============================] - 0s 950us/step - loss: 0.4427 - accuracy: 0.8080\n",
    "    Epoch 7/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8101\n",
    "    Epoch 8/100\n",
    "    250/250 [==============================] - 0s 996us/step - loss: 0.4426 - accuracy: 0.8101\n",
    "    Epoch 9/100\n",
    "    250/250 [==============================] - 0s 947us/step - loss: 0.4403 - accuracy: 0.8084\n",
    "    Epoch 10/100\n",
    "    250/250 [==============================] - 0s 945us/step - loss: 0.4385 - accuracy: 0.8102\n",
    "    Epoch 11/100\n",
    "    250/250 [==============================] - 0s 914us/step - loss: 0.4356 - accuracy: 0.8114\n",
    "    Epoch 12/100\n",
    "    250/250 [==============================] - 0s 967us/step - loss: 0.4376 - accuracy: 0.8091\n",
    "    Epoch 13/100\n",
    "    250/250 [==============================] - 0s 922us/step - loss: 0.4371 - accuracy: 0.8089\n",
    "    Epoch 14/100\n",
    "    250/250 [==============================] - 0s 959us/step - loss: 0.4332 - accuracy: 0.8123\n",
    "    Epoch 15/100\n",
    "    250/250 [==============================] - 0s 953us/step - loss: 0.4304 - accuracy: 0.8135\n",
    "    Epoch 16/100\n",
    "    250/250 [==============================] - 0s 951us/step - loss: 0.4300 - accuracy: 0.8144\n",
    "    Epoch 17/100\n",
    "    250/250 [==============================] - 0s 905us/step - loss: 0.4262 - accuracy: 0.8131\n",
    "    Epoch 18/100\n",
    "    250/250 [==============================] - 0s 955us/step - loss: 0.4237 - accuracy: 0.8130\n",
    "    Epoch 19/100\n",
    "    250/250 [==============================] - 0s 914us/step - loss: 0.4190 - accuracy: 0.8163\n",
    "    Epoch 20/100\n",
    "    250/250 [==============================] - 0s 965us/step - loss: 0.4126 - accuracy: 0.8229\n",
    "    Epoch 21/100\n",
    "    250/250 [==============================] - 0s 906us/step - loss: 0.4043 - accuracy: 0.8278\n",
    "    Epoch 22/100\n",
    "    250/250 [==============================] - 0s 904us/step - loss: 0.4019 - accuracy: 0.8294\n",
    "    Epoch 23/100\n",
    "    250/250 [==============================] - 0s 964us/step - loss: 0.3946 - accuracy: 0.8313\n",
    "    Epoch 24/100\n",
    "    250/250 [==============================] - 0s 924us/step - loss: 0.3904 - accuracy: 0.8303\n",
    "    Epoch 25/100\n",
    "    250/250 [==============================] - 0s 919us/step - loss: 0.3890 - accuracy: 0.8325\n",
    "    Epoch 26/100\n",
    "    250/250 [==============================] - 0s 908us/step - loss: 0.3858 - accuracy: 0.8331\n",
    "    Epoch 27/100\n",
    "    250/250 [==============================] - 0s 914us/step - loss: 0.3845 - accuracy: 0.8330\n",
    "    Epoch 28/100\n",
    "    250/250 [==============================] - 0s 924us/step - loss: 0.3845 - accuracy: 0.8347\n",
    "    Epoch 29/100\n",
    "    250/250 [==============================] - 0s 938us/step - loss: 0.3785 - accuracy: 0.8370\n",
    "    Epoch 30/100\n",
    "    250/250 [==============================] - 0s 968us/step - loss: 0.3795 - accuracy: 0.8370\n",
    "    Epoch 31/100\n",
    "    250/250 [==============================] - 0s 931us/step - loss: 0.3775 - accuracy: 0.8338\n",
    "    Epoch 32/100\n",
    "    250/250 [==============================] - 0s 928us/step - loss: 0.3751 - accuracy: 0.8349\n",
    "    Epoch 33/100\n",
    "    250/250 [==============================] - 0s 889us/step - loss: 0.3758 - accuracy: 0.8385\n",
    "    Epoch 34/100\n",
    "    250/250 [==============================] - 0s 883us/step - loss: 0.3733 - accuracy: 0.8385\n",
    "    Epoch 35/100\n",
    "    250/250 [==============================] - 0s 915us/step - loss: 0.3748 - accuracy: 0.8385\n",
    "    Epoch 36/100\n",
    "    250/250 [==============================] - 0s 978us/step - loss: 0.3709 - accuracy: 0.8418\n",
    "    Epoch 37/100\n",
    "    250/250 [==============================] - 0s 919us/step - loss: 0.3679 - accuracy: 0.8407\n",
    "    Epoch 38/100\n",
    "    250/250 [==============================] - 0s 923us/step - loss: 0.3720 - accuracy: 0.8419\n",
    "    Epoch 39/100\n",
    "    250/250 [==============================] - 0s 893us/step - loss: 0.3676 - accuracy: 0.8438\n",
    "    Epoch 40/100\n",
    "    250/250 [==============================] - 0s 949us/step - loss: 0.3676 - accuracy: 0.8426\n",
    "    Epoch 41/100\n",
    "    250/250 [==============================] - 0s 892us/step - loss: 0.3658 - accuracy: 0.8455\n",
    "    Epoch 42/100\n",
    "    250/250 [==============================] - 0s 940us/step - loss: 0.3653 - accuracy: 0.8451\n",
    "    Epoch 43/100\n",
    "    250/250 [==============================] - 0s 916us/step - loss: 0.3667 - accuracy: 0.8429\n",
    "    Epoch 44/100\n",
    "    250/250 [==============================] - 0s 988us/step - loss: 0.3614 - accuracy: 0.8474\n",
    "    Epoch 45/100\n",
    "    250/250 [==============================] - 0s 925us/step - loss: 0.3592 - accuracy: 0.8487\n",
    "    Epoch 46/100\n",
    "    250/250 [==============================] - 0s 949us/step - loss: 0.3592 - accuracy: 0.8501\n",
    "    Epoch 47/100\n",
    "    250/250 [==============================] - 0s 923us/step - loss: 0.3576 - accuracy: 0.8522\n",
    "    Epoch 48/100\n",
    "    250/250 [==============================] - 0s 903us/step - loss: 0.3579 - accuracy: 0.8506\n",
    "    Epoch 49/100\n",
    "    250/250 [==============================] - 0s 918us/step - loss: 0.3571 - accuracy: 0.8514\n",
    "    Epoch 50/100\n",
    "    250/250 [==============================] - 0s 936us/step - loss: 0.3553 - accuracy: 0.8522\n",
    "    Epoch 51/100\n",
    "    250/250 [==============================] - 0s 918us/step - loss: 0.3549 - accuracy: 0.8543\n",
    "    Epoch 52/100\n",
    "    250/250 [==============================] - 0s 925us/step - loss: 0.3525 - accuracy: 0.8546\n",
    "    Epoch 53/100\n",
    "    250/250 [==============================] - 0s 931us/step - loss: 0.3518 - accuracy: 0.8533\n",
    "    Epoch 54/100\n",
    "    250/250 [==============================] - 0s 870us/step - loss: 0.3530 - accuracy: 0.8549\n",
    "    Epoch 55/100\n",
    "    250/250 [==============================] - 0s 952us/step - loss: 0.3519 - accuracy: 0.8553\n",
    "    Epoch 56/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.8529\n",
    "    Epoch 57/100\n",
    "    250/250 [==============================] - 0s 936us/step - loss: 0.3503 - accuracy: 0.8555\n",
    "    Epoch 58/100\n",
    "    250/250 [==============================] - 0s 909us/step - loss: 0.3498 - accuracy: 0.8585\n",
    "    Epoch 59/100\n",
    "    250/250 [==============================] - 0s 999us/step - loss: 0.3497 - accuracy: 0.8569\n",
    "    Epoch 60/100\n",
    "    250/250 [==============================] - 0s 958us/step - loss: 0.3493 - accuracy: 0.8554\n",
    "    Epoch 61/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8554\n",
    "    Epoch 62/100\n",
    "    250/250 [==============================] - 0s 986us/step - loss: 0.3478 - accuracy: 0.8584\n",
    "    Epoch 63/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8571\n",
    "    Epoch 64/100\n",
    "    250/250 [==============================] - 0s 952us/step - loss: 0.3472 - accuracy: 0.8560\n",
    "    Epoch 65/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8596\n",
    "    Epoch 66/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8594\n",
    "    Epoch 67/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8574\n",
    "    Epoch 68/100\n",
    "    250/250 [==============================] - 0s 971us/step - loss: 0.3466 - accuracy: 0.8559\n",
    "    Epoch 69/100\n",
    "    250/250 [==============================] - 0s 993us/step - loss: 0.3449 - accuracy: 0.8579\n",
    "    Epoch 70/100\n",
    "    250/250 [==============================] - 0s 938us/step - loss: 0.3473 - accuracy: 0.8553\n",
    "    Epoch 71/100\n",
    "    250/250 [==============================] - 0s 925us/step - loss: 0.3454 - accuracy: 0.8572\n",
    "    Epoch 72/100\n",
    "    250/250 [==============================] - 0s 943us/step - loss: 0.3432 - accuracy: 0.8579\n",
    "    Epoch 73/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8562\n",
    "    Epoch 74/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8581\n",
    "    Epoch 75/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8596\n",
    "    Epoch 76/100\n",
    "    250/250 [==============================] - 0s 979us/step - loss: 0.3428 - accuracy: 0.8593\n",
    "    Epoch 77/100\n",
    "    250/250 [==============================] - 0s 941us/step - loss: 0.3441 - accuracy: 0.8595\n",
    "    Epoch 78/100\n",
    "    250/250 [==============================] - 0s 930us/step - loss: 0.3437 - accuracy: 0.8575\n",
    "    Epoch 79/100\n",
    "    250/250 [==============================] - 0s 931us/step - loss: 0.3418 - accuracy: 0.8585\n",
    "    Epoch 80/100\n",
    "    250/250 [==============================] - 0s 948us/step - loss: 0.3414 - accuracy: 0.8579\n",
    "    Epoch 81/100\n",
    "    250/250 [==============================] - 0s 956us/step - loss: 0.3425 - accuracy: 0.8584\n",
    "    Epoch 82/100\n",
    "    250/250 [==============================] - 0s 945us/step - loss: 0.3412 - accuracy: 0.8605\n",
    "    Epoch 83/100\n",
    "    250/250 [==============================] - 0s 989us/step - loss: 0.3409 - accuracy: 0.8595\n",
    "    Epoch 84/100\n",
    "    250/250 [==============================] - 0s 962us/step - loss: 0.3418 - accuracy: 0.8583\n",
    "    Epoch 85/100\n",
    "    250/250 [==============================] - 0s 925us/step - loss: 0.3402 - accuracy: 0.8604\n",
    "    Epoch 86/100\n",
    "    250/250 [==============================] - 0s 905us/step - loss: 0.3395 - accuracy: 0.8577\n",
    "    Epoch 87/100\n",
    "    250/250 [==============================] - 0s 969us/step - loss: 0.3399 - accuracy: 0.8596\n",
    "    Epoch 88/100\n",
    "    250/250 [==============================] - 0s 926us/step - loss: 0.3393 - accuracy: 0.8608\n",
    "    Epoch 89/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8577\n",
    "    Epoch 90/100\n",
    "    250/250 [==============================] - 0s 954us/step - loss: 0.3397 - accuracy: 0.8585\n",
    "    Epoch 91/100\n",
    "    250/250 [==============================] - 0s 936us/step - loss: 0.3383 - accuracy: 0.8608\n",
    "    Epoch 92/100\n",
    "    250/250 [==============================] - 0s 933us/step - loss: 0.3409 - accuracy: 0.8562\n",
    "    Epoch 93/100\n",
    "    250/250 [==============================] - 0s 973us/step - loss: 0.3379 - accuracy: 0.8612\n",
    "    Epoch 94/100\n",
    "    250/250 [==============================] - 0s 950us/step - loss: 0.3377 - accuracy: 0.8606\n",
    "    Epoch 95/100\n",
    "    250/250 [==============================] - 0s 981us/step - loss: 0.3372 - accuracy: 0.8615\n",
    "    Epoch 96/100\n",
    "    250/250 [==============================] - 0s 973us/step - loss: 0.3388 - accuracy: 0.8604\n",
    "    Epoch 97/100\n",
    "    250/250 [==============================] - 0s 969us/step - loss: 0.3377 - accuracy: 0.8620\n",
    "    Epoch 98/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8594\n",
    "    Epoch 99/100\n",
    "    250/250 [==============================] - 0s 1ms/step - loss: 0.3381 - accuracy: 0.8601\n",
    "    Epoch 100/100\n",
    "    250/250 [==============================] - 0s 950us/step - loss: 0.3377 - accuracy: 0.8611\n",
    "\n",
    "Out\\[43\\]:\n",
    "\n",
    "    <keras.callbacks.History at 0x7fab842c1540>\n",
    "\n",
    "In \\[44\\]:\n",
    "\n",
    "    model.evaluate(xtest,ytest)\n",
    "\n",
    "    63/63 [==============================] - 0s 884us/step - loss: 0.3560 - accuracy: 0.8555\n",
    "\n",
    "Out\\[44\\]:\n",
    "\n",
    "    [0.35597389936447144, 0.8554999828338623]\n",
    "\n",
    "In \\[45\\]:\n",
    "\n",
    "    yp=model.predict(xtest)\n",
    "\n",
    "    63/63 [==============================] - 0s 872us/step\n",
    "\n",
    "In \\[47\\]:\n",
    "\n",
    "    yp[:5]\n",
    "\n",
    "Out\\[47\\]:\n",
    "\n",
    "    array([[0.5022228 ],\n",
    "           [0.09951729],\n",
    "           [0.0517025 ],\n",
    "           [0.05196378],\n",
    "           [0.24340843]], dtype=float32)\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    y_predicted=[]\n",
    "    for i in yp:\n",
    "        if i>0.5:\n",
    "            y_predicted.append(1)\n",
    "        else:\n",
    "            y_predicted.append(0)\n",
    "\n",
    "In \\[49\\]:\n",
    "\n",
    "    y_predicted[:10]\n",
    "\n",
    "Out\\[49\\]:\n",
    "\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "In \\[50\\]:\n",
    "\n",
    "    ytest[:10]\n",
    "\n",
    "Out\\[50\\]:\n",
    "\n",
    "    4345    1\n",
    "    5182    1\n",
    "    1597    0\n",
    "    4459    0\n",
    "    4168    0\n",
    "    1901    0\n",
    "    7738    0\n",
    "    1243    0\n",
    "    4445    0\n",
    "    8435    1\n",
    "    Name: Exited, dtype: int64\n",
    "\n",
    "In \\[52\\]:\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "In \\[54\\]:\n",
    "\n",
    "    print(classification_report(y_predicted,ytest))\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "               0       0.96      0.87      0.91      1765\n",
    "               1       0.43      0.71      0.54       235\n",
    "\n",
    "        accuracy                           0.86      2000\n",
    "       macro avg       0.69      0.79      0.73      2000\n",
    "    weighted avg       0.90      0.86      0.87      2000\n",
    "\n",
    "In \\[57\\]:\n",
    "\n",
    "    import seaborn as sn\n",
    "    import matplotlib.pyplot as plt\n",
    "    cm=tf.math.confusion_matrix(labels=ytest,predictions=y_predicted)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(cm, annot=True, fmt='d')\n",
    "\n",
    "Out\\[57\\]:\n",
    "\n",
    "    <AxesSubplot:>\n",
    "\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAiwAAAGbCAYAAADnUMu5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdpklEQVR4nO3de5xWVbnA8d9zuCiYCmiRAidvpGlmeQwt5VRaouYR7Zi3VFKKTkl2Mc3qGOekdbS8ZZJGgkImaWKJZXmINE+Z5K28pk6awYQicqmkhGHW+WM29EIzzDC+zLvX7N/Xz/7M3mutd/Z6P0Y8Ps9ae0dKCUmSpDL7p0ZPQJIkqTMGLJIkqfQMWCRJUukZsEiSpNIzYJEkSaXXd1PfYNXip9yGJDXAgO1HN3oKUmW1rGyOnrxfPf+u7bftTj06964ywyJJkkpvk2dYJEnSJta6utEz2OQMWCRJyl1qbfQMNjlLQpIkqfTMsEiSlLvW3p9hMWCRJClzyZKQJElS45lhkSQpd5aEJElS6VkSkiRJajwzLJIk5c4Hx0mSpNKzJCRJktR4ZlgkScqdu4QkSVLZ+eA4SZKkEjDDIklS7iwJSZKk0rMkJEmS1HhmWCRJyp0PjpMkSaVnSUiSJKnxzLBIkpQ7dwlJkqTSsyQkSZLUeGZYJEnKnSUhSZJUdin1/m3NloQkSVLpmWGRJCl3FVh0a8AiSVLuXMMiSZJKrwIZFtewSJKk0jPDIklS7irw8kMzLJIk5S611u/oRERMi4hFEfFwO31nRESKiG2L64iIyyKiKSIejIi9a8aOi4gni2NcZ/c1YJEkSRvjGuCQ9RsjYgRwMPCHmuZDgZHFMQG4ohg7BJgE7AuMAiZFxOAN3dSARZKk3LW21u/oRErpTmBJO12XAGcBqaZtLDAjtbkbGBQR2wFjgDkppSUppaXAHNoJgmq5hkWSpNzVcZdQREygLRuyxpSU0pROPjMWaE4p/SYiaruGAfNrrhcUbR21d8iARZIkrVUEJxsMUGpFxEDgs7SVgzYZAxZJknLX2AfH7QzsCKzJrgwH7o+IUUAzMKJm7PCirRl4+3rtd2zoJq5hkSQpdz24hmV9KaWHUkqvSintkFLagbbyzt4ppWeB2cDJxW6h/YDlKaWFwG3AwRExuFhse3DR1iEDFkmS1GURMRP4JbBrRCyIiPEbGH4r8BTQBHwT+AhASmkJcC5wT3F8oWjrkCUhSZIyl1LPPTgupXR8J/071Jwn4LQOxk0DpnX1vgYskiTlrgIvP7QkJEmSSs8MiyRJuavA25oNWCRJyp0lIUmSpMYzwyJJUu4sCUmSpNKzJCRJktR4ZlgkScqdJSFJklR6loQkSZIazwyLJEm5q0CGxYBFkqTcVWANiyUhSZJUemZYJEnKnSUhSZJUepaEJEmSGs8MiyRJubMkJEmSSs+SkCRJUuOZYZEkKXeWhCRJUulVIGCxJCRJkkrPDIskSblLqdEz2OQMWCRJyp0lIUmSpMYzwyJJUu4qkGExYJEkKXc+OE6SJKnxzLBIkpQ7S0KSJKn0KrCt2ZKQJEkqPTMskiTlzpKQJEkqvQoELJaEJElS6ZlhkSQpdxV4DosBiyRJmUut7hKSJElqODMskiTlrgKLbg1YJEnKXQXWsFgSkiRJpWeGRZKk3FVg0a0BiyRJuavAGhZLQpIk5a61tX5HJyJiWkQsioiHa9q+EhG/jYgHI+J7ETGopu8zEdEUEY9HxJia9kOKtqaIOLuz+xqwSJKkjXENcMh6bXOA16eU3gA8AXwGICJ2B44D9ig+8/WI6BMRfYDJwKHA7sDxxdgOGbBIkpS7lOp3dHqrdCewZL22/00ptRSXdwPDi/OxwHdSSi+llJ4GmoBRxdGUUnoqpbQS+E4xtkMGLJIk5a6OJaGImBAR99YcEzZyNqcCPyrOhwHza/oWFG0dtXfIRbeSJGmtlNIUYEp3PhsRnwNagG/XdVIYsFTOf37pYu78xa8YMngQ37/2SgAmT72WWbN/zOBBWwPwsQ+N41/fOmrtZxY+u4gjTvwQHzn1fZxywtG89NJKxp12JitXrWJ1y2re9Y4DmPiBkxryfaTeYOutt2LKNy5kjz12JaXEBz94Bqef/gFe+9qdARi09VYsW/4n9nnzwQ2eqUqrBNuaI+L9wOHAQSmtrS01AyNqhg0v2thAe7sMWCrmyMPexQn/fgSfPffCddpPOvZITjnh6HY/8+WvTWH0fvusve7fvx/TLjufgQMHsKqlhZM//ClG77cPe73+dZt07lJvdcnFX+C2227n2OMm0K9fPwYOHMAJ7/vw2v6vXPB5lv/pTw2coUqvwU+6jYhDgLOAt6WUVtR0zQaui4iLge2BkcCvgABGRsSOtAUqxwEnbOgeBiwVs88b96R54XNdHj/3zrsYtt2rGTBg87VtEcHAgQMAaGlpoaWlhYio+1ylKthqqy0ZfcC+nDr+4wCsWrWK5ctXrTPm6KP/jXeNOaYBs5P+UUTMBN4ObBsRC4BJtO0K2gyYU/x9cHdK6T9SSo9ExA3Ao7SVik5LKa0ufs9E4DagDzAtpfTIhu7bacASEbvRtnJ3zWKYZmB2Sumxjf6WKq2Zs25h9o/nssduIzlz4gfZeqstWbHir0y79rt889IvcfXMWeuMX716Ncecejp/aP4jx7/ncN6wx24NmrmUtx13/GcWL36BqVddwhvesDv33/8gn/jk51mx4q8AjD5gX55b9DxNTU83eKYqtR4sCaWUjm+neeoGxn8R+GI77bcCt3b1vhvcJRQRn6Ztq1HQlsJZk8aZuaGHvNSuML5qxsyuzkUNcuxR7+ZHN0xj1jWTeeU2Q/jK5d8EYPK0aznp2KPWZlNq9enTh1nTJzP3e9/ioUef4Mmnft/Ds5Z6h759+vCmN+3JN74xgzePGsOLL67g02dNXNt/7LFHcv31NzdwhspBam2t21FWnWVYxgN7pJTWyU8WtahHgPPb+1DtCuNVi59q/EogbdC2QwavPT/6iEM57cxJADz0yOPMuf3nXPz1qfz5Ly8SEWzWvz8nHH3E2vFbbfkKRu39Bn5+972M3GmHnp66lL0FzQtZsGAhv7rnAQBuuumHnHVmW8DSp08fjjryUEbtd2gjpyiVQmcBSytti2SeWa99u6JPvcDzi5fwym2HADD3Z3exy06vAWDGFX9fmDt56rUMHLA5Jxx9BEuWLqNv375steUr+NtLL/HLex7g1BPf25C5S7l77rnnWbDgj7z2tTvzxBO/48ADD+Cxx54A4J0Hjebxx5tobl7Y4Fmq9EqwS2hT6yxg+TgwNyKe5O8PePlnYBdgYkcfUnmdOel87nngQZYt+xMHHXkiHxl/Evc88CCPP/kUBAx79VAmnXX6Bn/H8y8s5XPnXcjq1lZSa2LMgaN5+/779tA3kHqfj33iHGZM/xr9+/fj6af/wPgPfBKAY44Zy3csB6krGrxLqCdE6uQxvBHxT7Q9Qrd20e09a1b5dsaSkNQYA7Yf3egpSJXVsrK5R7dOvnjeiXX7u3aL/7y2lNs+O90llFJqpe29AJIkqYwsCUmSpNIr8e6eevHlh5IkqfTMsEiSlDtLQpIkqfQqsEvIkpAkSSo9MyySJOXOkpAkSSq7Mr8DqF4sCUmSpNIzwyJJUu4sCUmSpNKrQMBiSUiSJJWeGRZJknJXgeewGLBIkpQ7S0KSJEmNZ4ZFkqTMpQpkWAxYJEnKXQUCFktCkiSp9MywSJKUuwo8mt+ARZKk3FkSkiRJajwzLJIk5a4CGRYDFkmSMpdS7w9YLAlJkqTSM8MiSVLuLAlJkqTSq0DAYklIkiSVnhkWSZIy57uEJElS+VUgYLEkJEmSSs8MiyRJuev9rxIyYJEkKXdVWMNiSUiSJJWeGRZJknJXgQyLAYskSbmrwBoWS0KSJKn0zLBIkpQ5F91KkqTya63j0YmImBYRiyLi4Zq2IRExJyKeLH4OLtojIi6LiKaIeDAi9q75zLhi/JMRMa6z+xqwSJKkjXENcMh6bWcDc1NKI4G5xTXAocDI4pgAXAFtAQ4wCdgXGAVMWhPkdMSARZKkzKXWVLej03uldCewZL3mscD04nw6cGRN+4zU5m5gUERsB4wB5qSUlqSUlgJz+McgaB0GLJIk5a6OJaGImBAR99YcE7owg6EppYXF+bPA0OJ8GDC/ZtyCoq2j9g656FaSpMylOm5rTilNAaa8jM+niKj7KmAzLJIk6eV6rij1UPxcVLQ3AyNqxg0v2jpq75ABiyRJuevBXUIdmA2s2ekzDri5pv3kYrfQfsDyonR0G3BwRAwuFtseXLR1yJKQJEmZq2dJqDMRMRN4O7BtRCygbbfP+cANETEeeAY4phh+K3AY0ASsAE4BSCktiYhzgXuKcV9IKa2/kHcdBiySJKnLUkrHd9B1UDtjE3BaB79nGjCtq/c1YJEkKXcVeJeQAYskSZnryZJQo7joVpIklZ4ZFkmSMleFDIsBiyRJmatCwGJJSJIklZ4ZFkmScpei0TPY5AxYJEnKnCUhSZKkEjDDIklS5lKrJSFJklRyloQkSZJKwAyLJEmZS+4SkiRJZWdJSJIkqQTMsEiSlDl3CUmSpNJLqdEz2PQsCUmSpNIzwyJJUuYsCUmSpNKrQsBiSUiSJJWeGRZJkjJXhUW3BiySJGXOkpAkSVIJmGGRJClzvktIkiSVnu8SkiRJKgEzLJIkZa7VkpAkSSq7KqxhsSQkSZJKzwyLJEmZq8JzWAxYJEnKXBWedGtJSJIklZ4ZFkmSMmdJSJIklV4VtjVbEpIkSaVnhkWSpMxV4TksBiySJGXOXUKSJEklYIZFkqTMVWHRrQGLJEmZq8IaFktCkiSp9MywSJKUORfdSpKk0mtNUbejMxHxiYh4JCIejoiZEbF5ROwYEfMioikiro+I/sXYzYrrpqJ/h+5+RwMWSZLUJRExDDgd2Cel9HqgD3AccAFwSUppF2ApML74yHhgadF+STGuWzZ5SWjP3Y/d1LeQ1I6hWwxq9BQk9ZAeXnTbFxgQEauAgcBC4EDghKJ/OvBfwBXA2OIc4Ebg8oiIlDa+iGWGRZKkzNWzJBQREyLi3ppjwpr7pJSagQuBP9AWqCwH7gOWpZRaimELgGHF+TBgfvHZlmL8Nt35ji66lSRJa6WUpgBT2uuLiMG0ZU12BJYB3wUO6Yl5mWGRJClzqY5HJ94JPJ1Sej6ltAq4CdgfGBQRa5Igw4Hm4rwZGAFQ9G8NvNCd72jAIklS5npwl9AfgP0iYmBEBHAQ8ChwO3B0MWYccHNxPru4puj/aXfWr4AlIUmSstdTi25TSvMi4kbgfqAFeIC28tEPge9ExHlF29TiI1OBb0VEE7CEth1F3WLAIkmSuiylNAmYtF7zU8Codsb+DXhvPe5rwCJJUuZaGz2BHmDAIklS5hK+/FCSJKnhzLBIkpS51gq8/NCARZKkzLVaEpIkSWo8MyySJGWuCotuDVgkScpcFbY1WxKSJEmlZ4ZFkqTMWRKSJEmlZ0lIkiSpBMywSJKUuSpkWAxYJEnKXBXWsFgSkiRJpWeGRZKkzLX2/gSLAYskSbnzXUKSJEklYIZFkqTMpUZPoAcYsEiSlLkqbGu2JCRJkkrPDIskSZlrjd6/6NaARZKkzFVhDYslIUmSVHpmWCRJylwVFt0asEiSlLkqPOnWkpAkSSo9MyySJGWuCo/mN2CRJClz7hKSJEkqATMskiRlrgqLbg1YJEnKXBW2NVsSkiRJpWeGRZKkzFVh0a0BiyRJmavCGhZLQpIkqfTMsEiSlLkqLLo1YJEkKXNVCFgsCUmSpNIzwyJJUuZSBRbdGrBIkpQ5S0KSJEklYMAiSVLmWut4dCYiBkXEjRHx24h4LCLeEhFDImJORDxZ/BxcjI2IuCwimiLiwYjYu7vf0YBFkqTMpToeXfBV4Mcppd2AvYDHgLOBuSmlkcDc4hrgUGBkcUwArujudzRgkSRJXRIRWwP/CkwFSCmtTCktA8YC04th04Eji/OxwIzU5m5gUERs1517G7BIkpS51qjfERETIuLemmNCza12BJ4Hro6IByLiqojYAhiaUlpYjHkWGFqcDwPm13x+QdG20dwlJElS5uq5SyilNAWY0kF3X2Bv4KMppXkR8VX+Xv5Z8/kUEXV/H6MZFkmS1FULgAUppXnF9Y20BTDPrSn1FD8XFf3NwIiazw8v2jaaAYskSZnrqV1CKaVngfkRsWvRdBDwKDAbGFe0jQNuLs5nAycXu4X2A5bXlI42iiUhSZIyV/f6y4Z9FPh2RPQHngJOoS0BckNEjAeeAY4pxt4KHAY0ASuKsd1iwCJJkrospfRrYJ92ug5qZ2wCTqvHfQ1YJEnKXKvvEpIkSWVXhXcJGbBIkpS5Hl7D0hDuEpIkSaVnhkWSpMy1ViDHYsAiSVLmqrCGxZKQJEkqPTMskiRlrvcXhAxYJEnKniUhSZKkEjDDIklS5nzSrSRJKr0qbGu2JCRJkkrPDIskSZnr/fkVAxZJkrLnLiFJkqQSMMMiSVLmqrDo1oBFkqTM9f5wxZKQJEnKgBkWSZIyV4VFtwYskiRlrgprWCwJSZKk0jPDIklS5np/fsWARZKk7FVhDYslIUmSVHpmWCRJylyqQFHIgEWSpMxZEpIkSSoBMyySJGWuCs9hMWCRJClzvT9csSQkSZIyYIZFkqTMWRKSJEml5y4h9Wqv3n4o02+6gh/83/Xccuf1nPTB4wA4c9Lp3PqL73LzHdfxtWu+zJZbvQKAt75tFLPmzGD2HTOZNWcG+x6wTyOnL2Xtoq+dy2+euJO5d31/nfZTPngCP5t3Cz+962Y+999nANC3b18u/fqX+Mkvvscdd89m4ic+0IAZS41lhqXCVre0cMGkS3n0ocfZYouBzPrJDO762Tzu+tk8Lj5vMqtXr+aMcyYy4WPv56JzL2fpC8v48ImfZNFzixm5285cdf1lvG2vdzf6a0hZumHm97n6m9fx1Sv/Z23bWw8YxZjDDuRdo9/DypWr2GbbIQAcfuQY+m/Wj3fufxSbD9icO+6ezfdvvJUF8//YqOmrZKrw4DgzLBX2/KIXePShxwF48cUV/O6J3zN0u1fyizvmsXr1agB+c9/DvHr7oQA89vATLHpuMQBP/vZ3bLb5ZvTr368xk5cyN++u+1i2dPk6bSefeiyTL72KlStXAfDC4iUApJQYOHAgffr0YcDmm7Fq5Sr+8ucXe3zOKq/WOh5lZcAiAIaN2I7X7bkrv7nvkXXa//34I7hz7l3/MH7M4Qfy6EOPs6r4P1ZJL99Ou+zAqLf8C7fMmcmNP7iGvd70egB+ePP/smLFCh747R386qGfcOXl17Bs2fJOfpvUu3Q7YImIUzbQNyEi7o2Ie5f99fnu3kI9ZOAWA7hs2gX8zzkX8+Jf/v5fbR/6+Cm0rG7hlht/tM74XXbdiTM+/1EmfepLPT1VqVfr07cPgwZvzb+963jO+/xFXHn1RQC88V/2ZPXqVvZ+3TvY741j+NBp4/jn1wxv8GxVJqmO/5TVy8mw/HdHHSmlKSmlfVJK+wwa8MqXcQttan379uGyaRdwy6wfM+eHt69tP+rYw3nHwQdw5ofPWWf80O1exeXXfJlPT5zE/N839/R0pV5tYfNz/OiWnwDw6/sforW1lSHbDOaoo9/NHXN/TktLCy8sXsI98x5grzft0eDZqkwqXxKKiAc7OB4ChvbQHLUJnXfpOfzuid9zzZXXrW074B1vYfzEk/jwSWfwt7++tLZ9y61ewTeuu4SLzpvMA796sBHTlXq1226dy1tHjwJgp51fQ//+/VjywlKaFyxk/9H7AjBg4AD23mcvmp58upFTlXpcpNRx+icingPGAEvX7wLuSilt39kNdnvVm8ubX6q4vffdi+tuuYrHH32S1ta2f02XfHEyn/vSp+jfv//aBYG/ue8h/uvM8/mPT5zKhNPfzzNPz1/7O8YfM5Eli9f/n4fK4M+rVjR6CtqAyVd9hbfs/2aGbDOIxYte4MLzJzPr+lu46PJz2WPP3Vi1chXnnnMhv/i/eQzcYiCXXH4eI3fdmYjg+uu+x5Vfu7rRX0Eb0Lz0kejJ+530mvfU7e/abz1zU4/Ovas6C1imAlenlH7eTt91KaUTOruBAYvUGAYsUuP0dMByYh0DlmtLGrBs8DksKaXxG+jrNFiRJEmqB7c1S5KUuVZS3Y6uiIg+EfFARPyguN4xIuZFRFNEXB8R/Yv2zYrrpqJ/h+5+RwMWSZIy14BtzR8DHqu5vgC4JKW0C23rXtdUaMYDS4v2S4px3WLAIkmSuiwihgPvBq4qrgM4ELixGDIdOLI4H1tcU/QfVIzfaAYskiRlrp7PYal9+GtxTFjvdpcCZ/H3x7ZsAyxLKbUU1wuAYcX5MGA+QNG/vBi/0Xz5oSRJmevq2pOuSClNAaa01xcRhwOLUkr3RcTb63bTLjBgkSRJXbU/cEREHAZsDmwFfBUYFBF9iyzKcGDNo9CbgRHAgojoC2wNvNCdG1sSkiQpcz216Dal9JmU0vCU0g7AccBPU0rvA24Hji6GjQNuLs5nF9cU/T9NG3oA3AYYsEiSlLkSvEvo08AnI6KJtjUqU4v2qcA2RfsngbO7ewNLQpIkaaOllO4A7ijOnwJGtTPmb8B763E/AxZJkjLXzSpLVgxYJEnKXD13CZWVa1gkSVLpmWGRJClzL2OxbDYMWCRJytxGvAMoWwYskiRlzjUskiRJJWCGRZKkzLmtWZIklV4VFt1aEpIkSaVnhkWSpMy5S0iSJJWeu4QkSZJKwAyLJEmZc5eQJEkqPUtCkiRJJWCGRZKkzLlLSJIklV5rBdawWBKSJEmlZ4ZFkqTM9f78igGLJEnZc5eQJElSCZhhkSQpc1XIsBiwSJKUuSo86daSkCRJKj0zLJIkZc6SkCRJKr0qPOnWkpAkSSo9MyySJGWuCotuDVgkScpcFdawWBKSJEmlZ4ZFkqTMWRKSJEmlZ0lIkiSpBMywSJKUuSo8h8WARZKkzLVWYA2LJSFJklR6ZlgkScqcJSFJklR6loQkSZJKwAyLJEmZsyQkSZJKz5KQJElSISJGRMTtEfFoRDwSER8r2odExJyIeLL4Obhoj4i4LCKaIuLBiNi7u/c2YJEkKXOpjv90ogU4I6W0O7AfcFpE7A6cDcxNKY0E5hbXAIcCI4tjAnBFd7+jAYskSZlrTalux4aklBamlO4vzv8MPAYMA8YC04th04Eji/OxwIzU5m5gUERs153vaMAiSZI2WkTsALwJmAcMTSktLLqeBYYW58OA+TUfW1C0bTQDFkmSMlfPklBETIiIe2uOCevfLyJeAcwCPp5S+tM6c0kpQf23LblLSJKkzKXUWsfflaYAUzrqj4h+tAUr304p3VQ0PxcR26WUFhYln0VFezMwoubjw4u2jWaGRZIkdUlEBDAVeCyldHFN12xgXHE+Dri5pv3kYrfQfsDymtLRRjHDIklS5lp77sFx+wMnAQ9FxK+Lts8C5wM3RMR44BngmKLvVuAwoAlYAZzS3RsbsEiSlLnUQw+OSyn9HIgOug9qZ3wCTqvHvS0JSZKk0jPDIklS5nqwJNQwBiySJGWup0pCjWRJSJIklZ4ZFkmSMleFtzUbsEiSlLkuvLQwe5aEJElS6ZlhkSQpc1VYdGvAIklS5tzWLEmSSq8KGRbXsEiSpNIzwyJJUubc1ixJkkrPkpAkSVIJmGGRJClz7hKSJEmlZ0lIkiSpBMywSJKUOXcJSZKk0vPlh5IkSSVghkWSpMxZEpIkSaXnLiFJkqQSMMMiSVLmqrDo1oBFkqTMWRKSJEkqATMskiRlrgoZFgMWSZIy1/vDFUtCkiQpA1GFNJK6LyImpJSmNHoeUtX4Z09alxkWdWZCoycgVZR/9qQaBiySJKn0DFgkSVLpGbCoM9bQpcbwz55Uw0W3kiSp9MywSJKk0jNgkSRJpWfAonZFxCER8XhENEXE2Y2ej1QVETEtIhZFxMONnotUJgYs+gcR0QeYDBwK7A4cHxG7N3ZWUmVcAxzS6ElIZWPAovaMAppSSk+llFYC3wHGNnhOUiWklO4EljR6HlLZGLCoPcOA+TXXC4o2SZIawoBFkiSVngGL2tMMjKi5Hl60SZLUEAYsas89wMiI2DEi+gPHAbMbPCdJUoUZsOgfpJRagInAbcBjwA0ppUcaOyupGiJiJvBLYNeIWBAR4xs9J6kMfDS/JEkqPTMskiSp9AxYJElS6RmwSJKk0jNgkSRJpWfAIkmSSs+ARZIklZ4BiyRJKr3/B2Aro011x6d5AAAAAElFTkSuQmCC%0A\" class=\"jp-needs-light-background\" />\n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
